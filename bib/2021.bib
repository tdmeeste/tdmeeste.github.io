

@article{zaporojets2021eswa,
author = {Zaporojets, Klim and Bekoulis, Giannis and Deleu, Johannes and Demeester, Thomas and Develder, Chris},
title = {Solving arithmetic word problems by scoring equations with recursive neural networks},
journal = {Expert Syst. Appl.},
month = {15 Jul.},
year = {2021},
volume = {174},
doi = {10.1016/j.eswa.2021.114704},
pdf = {https://arxiv.org/pdf/2009.05639},
pubtype={a1}
}


# Neuro-Symbolic AI= Neural+ Logical+ Probabilistic AI (book chapter)
@incollection{NeSybookchapter2021,
  abstract     = {{There is a broad consensus that both learning and reasoning are essential to achieve true artificial intelligence. This has put the quest for neural-symbolic artificial intelligence (NeSy) high on the research agenda. In the past decade, neural networks have caused great advances in the field of machine learning. Conversely, the two most prominent frameworks for reasoning are logic and probability. While in the past they were studied by separate communities, a significant number of researchers has been working towards their integration, cf. the area of statistical relational artificial intelligence (StarAI). Generally, NeSy systems integrate logic with neural networks. However, probability theory has already been integrated with both logic (cf. StarAI) and neural networks. It therefore makes sense to consider the integration of logic, neural networks and probabilities. In this chapter, we first consider these three base paradigms separately. Then, we look at the well established integrations, NeSy and StarAI. Next, we consider the integration of all three paradigms as Neural Probabilistic Logic Programming, and exemplify it with the DeepProbLog framework. Finally, we discuss the limitations of the state of the art, and consider future directions based on the parallels between StarAI and NeSy.}},
  author       = {Manhaeve, R. and Marra, G. and Demeester, T. and Dumancic, S. and Kimmig, A. and De Raedt, L.},
  booktitle    = {{Neuro-symbolic artificial intelligence : the state of the art}},
  isbn         = {{9781643682440}},
  issn         = {{0922-6389}},
  language     = {{eng}},
  pages        = {{173--191}},
  publisher    = {{IOS press}},
  series       = {{Frontiers in Artificial Intelligence and Applications}},
  title        = {{Chapter 7 : Neuro-symbolic AI = neural + logical + probabilistic AI}},
  url          = {{http://dx.doi.org/10.3233/FAIA210354}},
  volume       = {{342}},
  year         = {{2021}},
}




# Exploration of block-wise dynamic sparseness
@article{hadifar2021prl,
author = {Hadifar, Amir and Deleu, Johannes and Develder, Chris and Demeester, Thomas},
title = {Exploration of block-wise dynamic sparseness},
journal = {Pattern Recognit. Lett.},
month = {Nov.},
year = {2021},
volume = {151},
pages = {187--192},
doi = {10.1016/j.patrec.2021.08.013},
pdf = {https://biblio.ugent.be/publication/8722050/file/8722051.pdf},
pubtype = {a1}
}



# Overly optimistic prediction results on imbalanced data: a case study of flaws and benefits when applying over-sampling 2021
@article{Vandewiele2021_AIM,
  abstract     = {Information extracted from electrohysterography recordings could potentially prove to be an interesting additional source of information to estimate the risk on preterm birth. Recently, a large number of studies have reported near-perfect results to distinguish between recordings of patients that will deliver term or preterm using a public resource, called the Term/Preterm Electrohysterogram database. However, we argue that these results are overly optimistic due to a methodological flaw being made. In this work, we focus on one specific type of methodological flaw: applying over-sampling before partitioning the data into mutually exclusive training and testing sets. We show how this causes the results to be biased using two artificial datasets and reproduce results of studies in which this flaw was identified. Moreover, we evaluate the actual impact of over-sampling on predictive performance, when applied prior to data partitioning, using the same methodologies of related studies, to provide a realistic view of these methodologies' generalization capabilities. We make our research reproducible by providing all the code under an open license.},
  articleno    = {101987},
  author       = {Vandewiele, Gilles and Dehaene, Isabelle and Kovács, György and Sterckx, Lucas and Janssens, Olivier and Ongenae, Femke and De Backere, Femke and De Turck, Filip and Roelens, Kristien and Decruyenaere, Johan and Van Hoecke, Sofie and Demeester, Thomas},
  issn         = {0933-3657},
  journal      = {Artificial Intelligence in Medicine},
  keywords     = {Medicine (miscellaneous),Artificial Intelligence,Preterm birth risk estimation,Over-sampling,Electrohysterography,SMOTE},
  title        = {Overly optimistic prediction results on imbalanced data: a case study of flaws and benefits when applying over-sampling},
  url          = {http://dx.doi.org/10.1016/j.artmed.2020.101987},
  volume       = {111},
  year         = {2021},
  pdf   = {https://arxiv.org/pdf/2001.06296},
  pubtype = {a1}
}


@article{ManhaeveDumancic2021_DPL2,
year = {2021},
journal = {Artificial Intelligence},
volume = {298},
pages = {103504},
issn = {0004-3702},
doi = {https://doi.org/10.1016/j.artint.2021.103504},
url = {https://www.sciencedirect.com/science/article/pii/S0004370221000552},
title = {{Neural Probabilistic Logic Programming in DeepProbLog}},
author = {Manhaeve, Robin and Duman\v{c}i\'c, Sebastijan and Kimmig, Angelika and Demeester, Thomas and De Raedt, Luc},
abstract = {We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.},
keywords = {},
pdf = {https://arxiv.org/pdf/1907.08194},
pubtype={a1}
}


# DWIE: An entity-centric dataset for multi-task document-level information extraction
@article{Zaporojets2021_DWIE,
title = {{DWIE}: An entity-centric dataset for multi-task document-level information extraction},
journal = {Information Processing \& Management},
volume = {58},
number = {4},
pages = {102563},
year = {2021},
issn = {0306-4573},
doi = {https://doi.org/10.1016/j.ipm.2021.102563},
author = {Klim Zaporojets and Johannes Deleu and Chris Develder and Thomas Demeester},
keywords = {Named entity recognition, Entity linking, Relation extraction, Coreference resolution, Joint models, Graph Neural Networks},
abstract = {This paper presents DWIE, the ``Deutsche Welle corpus for Information Extraction'', a newly created multi-task dataset that combines four main Information Extraction (IE) annotation subtasks: (i) Named Entity Recognition (NER), (ii) Coreference Resolution, (iii) Relation Extraction (RE), and (iv) Entity Linking. DWIE is conceived as an entity-centric dataset that describes interactions and properties of conceptual entities on the level of the complete document. This contrasts with currently dominant mention-driven approaches that start from the detection and classification of named entity mentions in individual sentences. Further, DWIE presented two main challenges when building and evaluating IE models for it. First, the use of traditional mention-level evaluation metrics for NER and RE tasks on entity-centric DWIE dataset can result in measurements dominated by predictions on more frequently mentioned entities. We tackle this issue by proposing a new entity-driven metric that takes into account the number of mentions that compose each of the predicted and ground truth entities. Second, the document-level multi-task annotations require the models to transfer information between entity mentions located in different parts of the document, as well as between different tasks, in a joint learning setting. To realize this, we propose to use graph-based neural message passing techniques between document-level mention spans. Our experiments show an improvement of up to 5.5 F1 percentage points when incorporating neural graph propagation into our joint model. This demonstrates DWIEâ's potential to stimulate further research in graph neural networks for representation learning in multi-task IE. We make DWIE publicly available at https://github.com/klimzaporojets/DWIE.},
pdf = {https://arxiv.org/pdf/2009.12626},
pubtype={a1}
}


# JobBERT: Understanding job titles through skills
@inproceedings{decorte2021feast,
author = {Decorte, Jens-Joris and Van Hautte, Jeroen and Demeester, Thomas and Develder, Chris},
title = {JobBERT: Understanding job titles through skills},
booktitle = {Proc. Int. Workshop Fair, Effective and Sustainable Talent at ECML-PKDD (FEAST 2021)},
month = {13--17 Sep.},
year = {2021},
pdf = {https://arxiv.org/pdf/2109.09605}
}



@inproceedings{verlinden2021,
author = {Verlinden, Severine and Zaporojets, Klim and Deleu, Johannes and Demeester, Thomas and Develder, Chris},
title = {Injecting knowledge base information into end-to-end joint entity and relation extraction and coreference resolution},
booktitle = {Findings of the ACL: ACL-IJCNLP 2021},
month = {1--6 Aug.},
year = {2021},
doi = {10.18653/v1/2021.findings-acl.171},
url = {https://aclanthology.org/2021.findings-acl.171/},
pdf = {https://arxiv.org/pdf/2107.02286}
}





# A Million Tweets Are Worth a Few Points: Tuning Transformers for Customer Service Tasks
@inproceedings{hadifar2021naacl,
author = {Hadifar, Amir and Labat, Sofie and Hoste, Veronique and Develder, Chris and Demeester, Thomas},
title = {A million tweets are worth a few points: Tuning transformers for customer support tasks},
booktitle = {Proc. Ann. Conf. North American Chapter Assoc. Comp. Linguist. (NAACL 2021)},
month = {6--11 Jun.},
year = {2021},
doi = {10.18653/v1/2021.naacl-main.21},
pdf = {https://arxiv.org/pdf/2104.07944}
}


# A Simple Geometric Method for Cross-Lingual Linguistic Transformations with Pre-trained Autoencoders
@inproceedings{deraedt2021emnlp,
author = {De Raedt, Maarten and Godin, Fréderic and Buteneers, Pieter and Develder, Chris and Demeester, Thomas},
title = {A simple geometric method for cross-lingual linguistic transformations with pre-trained autoencoders},
booktitle = {Proc. Conf. Empirical Methods in Natural Lang. Processing (EMNLP 2021)},
month = {7--11 Nov.},
year = {2021},
url = {https://aclanthology.org/2021.emnlp-main.792/},
pdf = {https://aclanthology.org/2021.emnlp-main.792.pdf}
}

# Lazy low-resource coreference resolution: A study on leveraging black-box translation tools
@inproceedings{bitew2021crac,
author = {Bitew, Semere Kiros and Deleu, Johannes and Develder, Chris and Demeester, Thomas},
title = {Lazy low-resource coreference resolution: A study on leveraging black-box translation tools},
booktitle = {Proc. 4th Workshop Comput. Models of Reference, Anaphora and Coreference (CRAC 2021) at EMNLP 2021},
month = {11 Nov.},
year = {2021},
pages = {1--6},
url = {https://aclanthology.org/2021.crac-1.6/},
pdf = {https://aclanthology.org/2021.crac-1.6.pdf}
}

