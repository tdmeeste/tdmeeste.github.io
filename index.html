---
layout: default
---


		<!-- Banner -->
			<section id="banner">
				<div class="inner">
					<h2>Thomas Demeester</h2>
					<p>Natural Language Processing, Deep Learning & AI</p>
					<ul class="actions">
						<li><a href="index.html" class="button big alt">Home</a></li>
						<li><a href="publications.html" class="button big alt">Publications</a></li>
						<li><a href="contact.html" class="button big alt">Contact</a></li>
					</ul>
					<ul class="actions">
						<li><a href="#bio" class="button alt">Bio</a></li>
						<!--<li><a href="#news" class="button alt">News</a></li>-->
						<li><a href="#recent" class="button alt">Recent work</a></li>
					</ul>
				</div>
			</section>


		<!-- Short bio -->
			<section id="bio" class="wrapper style1">
                <div class="container">
                    <header class="major">
                        <h2>Short bio & research interests</h2>
                    </header>
					<div class="row">
						<div class="3u">
							<section>
								<a href="#" class="image left"><img src="images/thomas.jpg" alt="" width="150"/></a>
							</section>
						</div>
						<div class="8u">
							<section>
                                <!--current research-->
								<p>
                                    Currently I am an assistant professor at the <a href="https://www.ugent.be/ea/idlab/en">Internet Technology and Data Science Lab (IDLab)</a>, Ghent University - imec, Belgium.
									I'm co-leading the Text-to-Knowledge research cluster with prof. <a href="http://users.atlantis.ugent.be/cdvelder/">Chris Develder</a>,
									where we work on natural language processing in general, with focus on information extraction for applications in the media and medical domain.
									Some of our recent work includes <a href="https://arxiv.org/abs/2009.12626">multi-task information extraction on the document level</a>,
									and <a href="https://www.sciencedirect.com/science/article/abs/pii/S1532046420301726">augmenting clinical predition models with text-based features</a>.
								</p>
                                <p>
                                    Besides sequence modeling with neural networks, I'm particularly interested in combining knowledge and neural network models.
									During a research visit in 2016 at <a href="http://mr.cs.ucl.ac.uk/">UCL NLP</a> lab at the <a href="https://www.ucl.ac.uk/">University College London</a>,
									I worked on <a href="https://arxiv.org/abs/1606.08359">injecting first-order logic into neural link prediction models</a>.
									Together with the <a href="https://dtai.cs.kuleuven.be/">DTAI</a> research group at <a href="https://www.kuleuven.be/english/">KULeuven</a>,
									I have been working on the <a href="https://arxiv.org/abs/1805.10872">combination of neural networks and probabilistic logic programs</a>.
									These research topics fit within my general interest in joining deep learning models with systems better suited to reason. 					</p>
								<p>
									Since I'm interested in AI in general, I'm having fun with a couple of side tracks. In combining my deep learning experience
									with my electrical engineering background, I worked on <a href="https://arxiv.org/pdf/1911.09431.pdf">system identification
									from unequally spaced time series</a>. Some new work is coming up on time series in the clinical domain, using techniques from complexity science.
									Also in the clinical domain, we came across some consistent misuse of data augmentation techniques,
									and tried to explain the issues to the medical AI community in <a href="https://arxiv.org/abs/2001.06296">this work</a>.
                                </p>
                                <p>
                                    In 2005, I received my M.Sc. degree in electrical engineering at Ghent University, after finishing my final year and master thesis at <a href="https://www.ethz.ch/en.html">ETH Zurich</a>, Switzerland, where I worked as a student assistent.
									In 2009, funded by a grant from the <a href="http://www.fwo.be/en/">Research Foundation - Flanders</a> (FWO),
									I obtained my Ph.D. at the Ghent University Department of Information Technology, under the supervision of prof. Daniel De Zutter, in the area of computational electromagnetics.
                                    Shortly afterwards, I got involved in research on Information Retrieval, in collaboration with
									the <a href="https://www.utwente.nl/en/eemcs/db/">Database Group</a> at the <a href="https://www.utwente.nl/en/">University of Twente</a> in The Netherlands, and from there my interests moved to Natural Language Processing.
                                </p>
							</section>
							<!--<hr />-->
						</div>
					</div>
				</div>
			</section>

		<!-- News -->
		<!--
			<section id="news" class="wrapper style2">
				<header class="major">
					<h2>Latest News</h2>
					<!--<p>Amet nisi nunc lorem accumsan</p>-->
		-->
		<!--
				</header>
				<div class="container">
					<div class="column">
						<div class="0u">
                            <table style="">

                                <tr>
                                    <td>11/11/2019</td>
                                    <td><i>"System Identification with Time-Aware Neural Sequence Models"</i> accepted for <u>AAAI 2020</u>. <a href="https://arxiv.org/pdf/1911.09431.pdf">[pdf]</a> <a href="https://github.com/tdmeeste/TimeAwareRNN">[code]</a></td>
                                </tr>
                            </table>
						</div>
					</div>
				</div>
			</section>
		-->

		<!-- Recent work -->
			<section id="recent" class="wrapper style1">
				<header class="major">
					<h2>Recent Work</h2>
					<!--<p>Tempus adipiscing commodo ut aliquam blandit</p>-->
				</header>
				<div class="container">
					<div class="row">
						<div class="4u">
							<section class="special box">
                                <div class="image rounded">
                                    <img src="images/recent_work/snapshot_lorenz_uneven.png" alt="" />
                                </div>
                                <h4>System Identification <br/>with Time-Aware RNNs</h4>
                                <p><i>AAAI 2020</i></p>
								<p><a href="https://arxiv.org/pdf/1911.09431.pdf">[pdf]</a> <a href="https://github.com/tdmeeste/TimeAwareRNN">[code]</a></p>
                                <p>Provides a simple and natural extension of RNNs (in particular the GRU) towards unevenly sampled time series,
									for modeling stationary MIMO systems. <br/>It doesn't always need to be NLP ;)
								</p>
							</section>
						</div>
						<div class="4u">
							<section class="special box">
                                <div class="image rounded">
                                    <img src="images/recent_work/predefined_sparseness_slide.png" alt="" />
                                </div>
                                <h4>Predefined Sparseness <br/>in recurrent sequence models</h4>
                                <p><i>CoNLL 2018</i></p>
								<p><a href="https://arxiv.org/pdf/1808.08720.pdf">[pdf]</a> <a href="https://github.com/tdmeeste/SparseSeqModels">[code]</a></p>
                                <p>Proposes models with predefined sparseness in embeddings and recurrent layers.
								Predefined sparse RNNs are designed such that they are equivalent to several small dense RNNs in parallel, sharing inputs.
									Word embeddings can made "sparse" up front by reducing effective embedding sizes for less frequent words.
								</p>
							</section>
						</div>
						<div class="4u">
							<section class="special box">
                                <div class="image rounded">
                                    <img src="images/recent_work/snapshot_DeepProbLog.png" alt="" />
                                </div>
								<h4>DeepProbLog: <br/>Neural Probabilistic Logic Programming</h4>
                                <p><i>NeurIPS 2018</i></p>
                                <p><a href="https://arxiv.org/abs/1805.10872">[pdf]</a></p>
								<p>Presents techniques for combining neural networks and probabilistic logic programs, by introducing neural predicates at the
									input of a ProbLog module and calculating loss gradients at the interface between both, to be used for
									backpropagation through the neural networks.</p>
							</section>
						</div>
						<div class="4u">
							<section class="special box">
                                <div class="image rounded">
                                    <img src="images/recent_work/snapshot_ExplainingChar.png" alt="" />
                                </div>
								<h4>Explaining Character-Aware Neural Networks <br/>for Word-Level Prediction:<br/>Do They Discover Linguistic Rules?</h4>
                                <p><i>EMNLP 2018</i></p>
								<p><a href="https://arxiv.org/pdf/1808.09551.pdf">[pdf]</a> <a href="https://github.com/FredericGodin/ContextualDecomposition-NLP">[code]</a></p>
								<p>Introduces Contextual Decomposition for CNNs and investigates how BiLSTM/CNN models capture morphological rules.</p>
							</section>
						</div>
					</div>
				</div>
			</section>


